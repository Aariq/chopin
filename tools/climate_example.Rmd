---
title: "Extracting Weather/Climate Geospatial Data with `chopin`"
author: Insang Song
date: today
output:
  html:
    embed-resources: true
---

<!-- header change
---
title: "Extracting Weather/Climate Geospatial Data with `chopin`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Extracting Weather/Climate Geospatial Data with `chopin`}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
-->

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, eval = FALSE)
```

<!--
quarto callout-notes
::: {.callout-note}
Note that there are five types of callouts, including:
`note`, `warning`, `important`, `tip`, and `caution`.
:::
-->


# Introduction

This document demonstrates to parallelize weather/climate geospatial data processing with `chopin` and what cases users may take advantage of parallel processing or not. We will cover the following formats:

We consider TerraClimate and PRISM data which have its own data format each. [Parameter-elevation Regressions on Independent Slopes Model (PRISM)](https://prism.oregonstate.edu) is a high-resolution (800-1,000 meters) gridded climate dataset available in the BIL (band interleaved by line) format which is readable with GDAL. TerraClimate is a high-resolution (5 km) gridded climate dataset in the NetCDF format which is readable with GDAL.

|     Data     | Source                  | Resolution     | File format |
|:------------:|:------------------------|:---------------|:------------|
| TerraClimate | University of Idaho     | 0.0417 degrees | NetCDF      |
|    PRISM     | Oregon State University | 0.0083 degrees | BIL      |

The spatial resolution of TerraClimate data commensurates 5 km in the equator, whereas that of PRISM data is approximately 1 km. The data are available in the NetCDF format which is readable with GDAL. We will use the `terra` package to read the data.

## Prepare target datasets

We will consider the populated places centroids in the mainland United States (i.e., excluding Alaska, Hawaii, and the territories). We will use the `tigris` package to download the data.

|                Data                | Number of features | Source                                    | Type    |
|:-------------------:|:----------------|:----------------|:----------------|
|          Census places          | 31,377             | US Census Bureau                          | Polygon |
|            Block groups            | 238,193            | US Census Bureau                          | Polygon |
| Grid points in the southeastern US | 1,204,904          | Author, US Census Bureau (state polygons) | Point   |

```{r}
#| label: load packages
pkgs <-
  c("chopin", "terra", "stars", "sf", "future", "doFuture", "dplyr", "parallelly", "tigris", "tictoc")
invisible(sapply(pkgs, library, character.only = TRUE, quietly = TRUE))
options(tigris_use_cache = TRUE, sf_use_s2 = FALSE)

```


## Download and preprocess {.tabset}

### Download
```{r}
#| label: download populated places and cleaning
#| eval: FALSE
# populated places
state <- tigris::states(year = 2022)
statemain <-
  state[!state$STUSPS %in% c("AK", "HI", "PR", "VI", "GU", "MP", "AS"), ]
target_states <- statemain$GEOID

popplace <-
  lapply(target_states, function(x) tigris::places(x, year = 2022)) %>%
  do.call(rbind, .)
saveRDS(popplace, "./input/populated_place_2022.rds", compress = "xz")
```

### Read
```{r}
#| label: mainland populated places
state <- tigris::states(year = 2022)
statemain <-
  state[!state$STUSPS %in% c("AK", "HI", "PR", "VI", "GU", "MP", "AS"), ]
target_states <- statemain$GEOID

# prepared populated places
popplace <- readRDS("./input/populated_place_2022.rds")

# generate circular buffers with 10 km radius
popplacep <- sf::st_centroid(popplace, of_largest_polygon = TRUE) %>%
  sf::st_transform("EPSG:5070")
popplacep2 <- sf::st_centroid(popplace, of_largest_polygon = TRUE)
popplaceb <- sf::st_buffer(popplacep, dist = units::set_units(10, "km"))

```



# TerraClimate

TerraClimate data are provided in yearly NetCDF files, each of which contains monthly layers. We will read the data with the `terra` package and preprocess the data to extract the annual mean and sum of the bands by types of columns. 

```{r TerraClimate-read}
# wbd
ext_mainland <- c(xmin = -126, xmax = -64, ymin = 22, ymax = 51)
ext_mainland <- terra::ext(ext_mainland)

path_tc <- file.path("input", "terraClimate")
path_tc_files <- list.files(
  path = path_tc, pattern = "*.nc$",
  full.names = TRUE
)
path_tc_files
```


## Preprocessing
Fourteen variables are available in TerraClimate data. The table below is from [the TerraClimate website](http://www.climatologylab.org/terraclimate-variables.html).

| Variable | Description | Units |
|:--------:|:------------|:------|
| aet      | Actual Evapotranspiration, monthly total | mm |
| def      | Climate Water Deficit, monthly total | mm |
| PDSI     | Palmer Drought Severity Index, at end of month | unitless |
| pet      | Potential evapotranspiration, monthly total | mm |
| ppt      | Precipitation, monthly total | mm |
| q        | Runoff, monthly total | mm |
| soil     | Soil Moisture, total column - at end of month | mm |
| srad     | Downward surface shortwave radiation | W/m<sup>2</sup> |
| swe      | Snow water equivalent - at end of month | mm |
| tmax     | Max Temperature, average for month | C |
| tmin     | Min Temperature, average for month | C |
| vap      | Vapor pressure, average for month | kPa |
| vpd      | Vapor Pressure Deficit, average for month | kpa |
| ws       | Wind speed, average for month | m/s |

The variables can be categorized into two types: those that will be summed and those that will be averaged.

- Sum: `aet`, `def`, `pet`, `ppt`, `q`, `soil`, and `swe`.
- Mean: `PDSI`, `srad`, `tmax`, `tmin`, `vap`, `vpd`, and `ws`.

Following that rationale, we will preprocess the data by summing the first seven layers and averaging the rest of the layers. The code blocks below follow "split-apply-combine" strategy. Note that `terra::tapp` aggregates layers by its attributes such as time or custom indices. 


```{r TerraClimate-preprocess}
# some bands should be summed
bandnames <- c(
  "aet", "def", "PDSI", "pet", "ppt", "q", "soil", "srad",
  "swe", "tmax", "tmin", "vap", "vpd", "ws"
)
bandnames_sorted <- sort(bandnames)

# single nc file, yearly aggregation by fun value
# band for summation
bandnames_sum <- c("aet", "def", "pet", "ppt", "q", "soil", "swe")

# band for averaging
bandnames_avg <- c("PDSI", "srad", "tmax", "tmin", "vap", "vpd", "ws")

# mean: temporally marginal pixel mean (i.e., monthly -> yearly)
# sum: temporally marginal pixel sum (i.e., monthly -> yearly)
# Preprocessed data are stored in
tictoc::tic("sum: 7 layers")
netcdf_read_sum <-
  split(bandnames_sum, bandnames_sum) |>
  lapply(function(x) {
    grep(paste0("(", x, ")"), path_tc_files, value = TRUE)
  }) |>
  lapply(function(x) {
    terra::tapp(terra::rast(x, win = ext_mainland, snap = "out"), index = "years", fun = "sum")
  })
netcdf_read_sum <- Reduce(c, netcdf_read_sum)
tictoc::toc()


names(netcdf_read_sum) <- rep(bandnames_sum, each = 23)
netcdf_read_sum

tictoc::tic("mean: 7 layers")
netcdf_read_mean <-
  split(bandnames_avg, bandnames_avg) |>
  lapply(function(x) {
    grep(paste0("(", x, ")"), path_tc_files, value = TRUE)
  }) |>
  lapply(function(x) {
    terra::tapp(terra::rast(x, win = ext_mainland, snap = "out"), index = "years", fun = "mean")
  }) |>
  Reduce(f = c, x = _)
tictoc::toc()
# 178.46 sec elapsed
names(netcdf_read_mean) <-
  sprintf("%s_%d", rep(bandnames_avg, each = 23), rep(2000:2022, 7))
netcdf_read_mean

```

> \[!WARNING\] Stacking raster data may take a long time and consume a large amount of memory depending on users' area of interest and data resolution. Users need to consider the memory capacity of the system before stacking rasters.

We have 14 data elements for 23 years with 12 months each. Below demonstrates the summary of the data layers that were averaged at circular buffer polygons with 10 kilometers (10,000 meters) radius. To leverage multiple cores in your machine, run `future::availableCores()` to check the number of cores and set the number of workers in `future::plan` accordingly. Typically, there are two logical cores in each physical core in modern central processing units. The number of workers should be set to up to the number of physical cores in the machine for optimal performance. The example below uses `future::multicore` which shares the memory across the workers.

```{r multiraster}
tic()
doFuture::registerDoFuture()
future::plan(future::multicore, workers = 8L)
grid_init <-
  chopin::par_make_gridset(
    popplacep2,
    mode = "grid",
    padding = 1e4,
    nx = 4L,
    ny = 2L
  )
multi_grid <-
  chopin::par_grid(
    grids = grid_init,
    fun_dist = chopin::extract_at_buffer,
    combine = dplyr::bind_rows,
    points = popplacep2,
    surf = netcdf_read_mean,
    id = "GEOID",
    func = "mean",
    radius = 1e4
  )
toc()

system.time(
  multi <-
    grep(
      paste0("(", paste(bandnames_avg, collapse = "|"), ")"),
      path_tc_files,
      value = TRUE
    ) %>%
    chopin::par_multirasters(
      filenames = .,
      fun_dist = chopin::extract_at_buffer,
      combine = dplyr::bind_cols,
      points = popplacep2,
      surf = rast(), # ignored
      id = "GEOID",
      func = "mean",
      radius = 1e4
    )
)


# single thread
system.time(
  single <-
  exactextractr::exact_extract(
    netcdf_read_mean,
    sf::st_as_sf(popplaceb),
    fun = "mean",
    stack_apply = TRUE,
    force_df = TRUE,
    append_cols = c("GEOID")
  )
)


```

> \[!CAUTION\] All Windows users and RStudio users (all platforms) will not be able to use `future::multicore` due to the restriction in `future` package. Please `future::multisession` instead and note that this option will runs slower than `future::multicore` case.


<!--
> \[!NOTE\] This is a note.

> \[!TIP\] This is a tip. (Supported since 14 Nov 2023)

> \[!IMPORTANT\] Crucial information comes here.

> \[!CAUTION\] Negative potential consequences of an action. (Supported since 14 Nov 2023)

> \[!WARNING\] Critical content comes here.

-->

# PRISM dataset

PRISM data are provided in monthly 30-year normal BIL files. Assuming that a user wants to summarize 30-year normal precipitation at 10 kilometers circular buffers of the geogrpahic centroids of [US Census Places](https://www2.census.gov/geo/pdfs/maps-data/data/tiger/tgrshp2023/TGRSHP2023_TechDoc_Ch4.pdf) (from p.26), we demonstrate the extraction process with the `chopin` package.


## Download and preprocess {.tabset}

### Download
```{r get-populated-places, eval = FALSE}
# populated places
# mainland states
state <- tigris::states(year = 2022)
statemain <-
  state[!state$STUSPS %in% c("AK", "HI", "PR", "VI", "GU", "MP", "AS"), ]
target_states <- statemain$GEOID

popplace <-
  lapply(target_states, function(x) tigris::places(x, year = 2022)) %>%
  do.call(rbind, .)
saveRDS(popplace, "./input/populated_place_2022.rds", compress = "xz")

```

### Read 
```{r PRISM-read}
bils <- list.files("input", "bil$", recursive = TRUE, full.names = TRUE)
bilssds <- terra::rast(bils[-13])
popplace2 <- sf::st_transform(popplace, crs = terra::crs(bilssds))
popplaceb2 <- sf::st_transform(popplaceb, crs = terra::crs(bilssds))


```


> \[!IMPORTANT\] `chopin::par_make_gridset` works the best with point datasets since each grid will clip the input features when parallelized. Polygon inputs will result in duplicate values in the output and lead to take longer to complete.


## Grid parallelization
In the same vein as the TerraClimate data, we will use the `chopin` package to parallelize the extraction process with grid strategy.

```{r prism-grid-compare}

exgrid <-
  chopin::par_make_gridset(
    popplacep2,
    mode = "grid",
    padding = 1e4,
    nx = 4L,
    ny = 2L
  )

doFuture::registerDoFuture()
future::plan(future::multicore, workers = 8L)
options(future.globals = TRUE)
system.time(
  exmulti <-
    chopin::par_grid(
      exgrid,
      fun_dist = chopin::extract_at_buffer,
      points = popplacep2,
      surf = bilssds,
      radius = units::set_units(1e4, "meter"),
      id = "GEOID",
      func = "mean"
    )
)

system.time(
  exsingle <-
    exactextractr::exact_extract(
      bilssds,
      popplaceb2,
      fun = "mean",
      stack_apply = TRUE,
      force_df = TRUE,
      append_cols = "GEOID",
      max_cells_in_memory = 2.14e9
    )
)


```


# Scaled up examples
## Larger buffer sizes
Examples above showed that the difference between the parallelized and single-threaded extraction process is not significant. We will increase the buffer size to 50 kilometers and compare the performance of the parallelized and single-threaded extraction process.

```{r prism-buffer-compare-50k}
# make buffers
popplaceb5 <- sf::st_buffer(popplacep, dist = units::set_units(50, "km")) %>%
  sf::st_transform(terra::crs(bilssds))

system.time(
  exsingle5 <-
    exactextractr::exact_extract(
      bilssds,
      popplaceb5,
      fun = "mean",
      stack_apply = TRUE,
      force_df = TRUE,
      append_cols = "GEOID",
      max_cells_in_memory = 2.14e9
    )
)


exgrid5k <-
  chopin::par_make_gridset(
    popplacep2,
    mode = "grid",
    padding = 5e4,
    nx = 4L,
    ny = 2L
  )


doFuture::registerDoFuture()
future::plan(future::multicore, workers = 8L)

system.time(
  exmulti5k <-
    chopin::par_grid(
      exgrid5k,
      fun_dist = chopin::extract_at_buffer,
      points = popplacep,
      surf = bilssds,
      radius = units::set_units(5e4, "meter"),
      id = "GEOID",
      func = "mean"
    )
)
```


> \[!NOTE\] The example above used strings of raster file paths for `surf` argument in `chopin::extract_at_buffer`. `terra::rast` at multiple file paths will return a `SpatRaster` with multiple layers **only** if the rasters have the same extent and resolution.


## Larger number of features





```{r}
## generate 1km grid points in the southeastern US States
stt <- tigris::states(year = 2020)
targ_states <- c("NC", "SC", "GA", "FL", "AL", "MS", "TN", "LA", "AR")
stt_targ <- stt[stt$STUSPS %in% targ_states, ]
plot(stt_targ$geometry)


stt_t <- sf::st_transform(stt_targ, "EPSG:5070")
stt_g <- sf::st_sample(stt_t, type = "regular", 1204934)
stt_g <- sf::st_as_sf(stt_g)
stt_g$pid <- seq(1, length(stt_g))
```

```{r}
stt_gb <- sf::st_buffer(stt_g, units::set_units(10, "km"))

tic()
single_2m <-
exactextractr::exact_extract(
  netcdf_read_sum,
  stt_gb,
  fun = "mean",
  stack_apply = TRUE,
  force_df = TRUE,
  max_cells_in_memory = 2.14e9
)
toc()

stt_gbg <-
  chopin::par_make_gridset(
    stt_g,
    mode = "grid",
    padding = 5e3,
    nx = 5L,
    ny = 5L
  )


doFuture::registerDoFuture()
future::plan(future::multicore, workers = 8L)
system.time(
  stt5k <-
    chopin::par_grid(
      stt_gbg,
      fun_dist = chopin::extract_at_poly,
      poly = stt_gb,
      surf = netcdf_read_sum,
      id = "pid",
      func = "mean"
    )
)

doFuture::registerDoFuture()
future::plan(future::multisession, workers = 8L)
system.time(
  stt5ks <-
    chopin::par_grid(
      stt_gbg,
      fun_dist = chopin::extract_at_poly,
      poly = stt_gb,
      surf = st_as_stars(netcdf_read_mean),
      id = "pid",
      func = "mean"
    )
)
```


## Finely resolved vector {.tabset}




### Download
```{r get-bg, eval = FALSE}
# set state=NULL and cb=TRUE will download the block groups for the entire US
bg <- tigris::block_groups(state = NULL, cb = TRUE, year = 2020)
sf::write_sf(bg, file.path("input", "Blockgroups_2020.gpkg"))
```

### Extract
```{r bg-extract}
## extract prism by par_hierarchy
bgsf <- sf::st_read("~/Blockgroups_2020.gpkg")
bgsf_main <- bgsf %>%
  dplyr::filter(!STATEFP %in% c("02", "15", "72", "66", "78", "60", "69"))

## extract prism at bg
system.time(
  exsingle <-
    exactextractr::exact_extract(
      bilssds,
      bgsf_main,
      fun = "mean",
      stack_apply = TRUE,
      force_df = TRUE,
      append_cols = "GEOID",
      max_cells_in_memory = 2.14e9
    )
)
#    user  system elapsed 
# 116.200   2.951 131.969


nmain <- c("AS", "AK", "HI", "PR", "VI", "GU", "MP")
stt_nmain <- stt[!stt$STUSPS %in% nmain, ]


doFuture::registerDoFuture()
future::plan(multicore, workers = 20L)
system.time(
  exhierarchy <-
    chopin::par_hierarchy(
      bgsf_main,
      split_level = "STATEFP",
      fun_dist = chopin::extract_at_poly,
      polys = bgsf_main,
      surf = bilssds,
      id = "GEOID"
    )
)

#  user  system elapsed 
# 7.255   2.367 205.995
# multicore (reflow)
  #  user  system elapsed 
  # 1.860   0.408 160.795 
# layer: sequential, multicore
#    user  system elapsed 
# 152.557   3.123 157.137 
```


## Finely resolved raster

We demonstrate the extraction process with the [CropScape](https://croplandcros.scinet.usda.gov) dataset which has a resolution of 30 meters. In this case, we use `frac` option of `exact_extract` which tabulates the fraction of the area of the cell category that is covered by the polygon after accounting for partial coverage of polygon segments over raster cells.

```{r fine-raster-cdl}


```


# Discussion: which strategy is better? stacked vs file-based parallelization

> \[!NOTE\] __It's up to the users' system specification and the size of the data.__

For faster computation, there are two strategies. One is parallelization which is implemented in `chopin` for example. The other strategy is to stack rasters and extract values at once. Adjustment of arguments in `exactextractr::exact_extract` will benefit many people who are dealing with sizable data that are able to be stacked. We compare the two strategies in terms of computation time.

Before proceeding, users need to consider the hardware specification. For example, memory and storage to leverage the maximal performance of `exact_extract`. Specifically speaking, memory capacity is crucial to store the stacked rasters in memory rather than to read the proxy of rasters from the disk as implemented in `terra`. `max_cells_in_memory` is a key argument to control the memory usage. The maximum possible value for this argument is $2^{31} - 1 = 2,147,483,647$, roughly `2.147e9`, as applied in the example above. As memory bandwidth is much faster than disk input/output specification, the stacked rasters with high `max_cells_in_memory` applied will run faster than file-based parallelization or the extraction with lower value of `max_cells_in_memory`. The performance does not come without a cost. The memory-intensive setting is not suitable for the system with limited memory, for example, in consumer laptops with around 16 GB of RAM.

## More tips to save time and memory

1. Always consider stacking rasters when you have a large number of separate raster files with the same resolution and extent if your machine's memory capacity allows. Reading multiple raster files with `terra::rast` will automatically stack them into a single `SpatRaster` object.

> \[!NOTE\] Stacking rasters usually takes the large amount of memory. Users need to consider the memory capacity of the system before stacking rasters.

2. Read vector data with `sf` package at first. It is a bit faster than `terra` and will save time for processing as `exactextractr` is designed to work with `sf` objects for vector inputs.
3. If users want to use `exactextractr` for the raster-vector overlay, pre-cropping the raster data may not help saving time for processing. This is because `exactextractr` will read the raster data with the extent of the vector data *ad hoc*.
4. If your analysis does not require the high precision of vector data, simplification of geometries (e.g., using [`rmapshaper`](https://github.com/ateucher/rmapshaper), i.e., `ms_simplify`) will result in considerable time savings.


## See also

-   [Garnett, R. (2023). Geospatial distributed processing with furrr](https://posit.co/blog/geospatial-distributed-processing-with-furrr/)
-   [Dyba, K. (n.d.). Parallel raster processing in *stars*](https://kadyb.github.io/stars-parallel/Tutorial.html)


