---
title: "Extracting Weather/Climate Geospatial Data with `chopin`"
author: Insang Song
date: today
output:
  html:
    embed-resources: true
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, eval = FALSE)
```

```{r}
#| eval: FALSE
library(reticulate)
use_python("../../.conda/envs/rapidsinit/bin/python")
```

<!--
quarto callout-notes
::: {.callout-note}
Note that there are five types of callouts, including:
`note`, `warning`, `important`, `tip`, and `caution`.
:::
-->


## Introduction

This document demonstrates to parallelize weather/climate geospatial data processing with `chopin` and what cases users may take advantage of parallel processing or not. We will cover the following formats:

We consider TerraClimate and PRISM data which have its own data format each. [Parameter-elevation Regressions on Independent Slopes Model (PRISM)](https://prism.oregonstate.edu) is a high-resolution (800-1,000 meters) gridded climate dataset available in the BIL (band interleaved by line) format which is readable with GDAL. TerraClimate is a high-resolution (5 km) gridded climate dataset in the NetCDF format which is readable with GDAL.

|     Data     | Source                  | Resolution     | File format |
|:------------:|:------------------------|:---------------|:------------|
| TerraClimate | University of Idaho     | 0.0417 degrees | NetCDF      |
|    PRISM     | Oregon State University | 0.0083 degrees | BIL      |

The spatial resolution of TerraClimate data commensurates 5 km in the equator, whereas that of PRISM data is approximately 1 km. The data are available in the NetCDF format which is readable with GDAL. We will use the `terra` package to read the data.

### Prepare target datasets

We will consider the populated places centroids in the mainland United States (i.e., excluding Alaska, Hawaii, and the territories). We will use the `tigris` package to download the data.

|                Data                | Number of features | Source                                    | Type    |
|:-------------------:|:----------------|:----------------|:----------------|
|          Populated places          | 31,377             | US Census Bureau                          | Polygon |
|            Block groups            | 238,193            | US Census Bureau                          | Polygon |
| Grid points in the southeastern US | 1,204,904          | Author, US Census Bureau (state polygons) | Point   |

```{r}
#| title: load packages
pkgs <-
  c("chopin", "terra", "stars", "sf", "future", "doFuture", "dplyr", "parallelly", "tigris", "tictoc")
invisible(sapply(pkgs, library, character.only = TRUE, quietly = TRUE))
options(tigris_use_cache = TRUE, sf_use_s2 = FALSE)

```


::: {.tabset}

## Download and export
```{r}
#| label: download populated places and cleaning
#| eval: FALSE
# populated places
state <- tigris::states(year = 2022)
statemain <-
  state[!state$STUSPS %in% c("AK", "HI", "PR", "VI", "GU", "MP", "AS"), ]
target_states <- statemain$GEOID

popplace <-
  lapply(target_states, function(x) tigris::places(x, year = 2022)) %>%
  do.call(rbind, .)
saveRDS(popplace, "./input/populated_place_2022.rds", compress = "xz")
```

## Read and preprocess
```{r}
#| label: mainland populated places
state <- tigris::states(year = 2022)
statemain <-
  state[!state$STUSPS %in% c("AK", "HI", "PR", "VI", "GU", "MP", "AS"), ]
target_states <- statemain$GEOID

# prepared populated places
popplace <- readRDS("./input/populated_place_2022.rds")

# generate circular buffers with 10 km radius
popplacep <- sf::st_centroid(popplace, of_largest_polygon = TRUE) %>%
  sf::st_transform("EPSG:5070")
popplacep2 <- sf::st_centroid(popplace, of_largest_polygon = TRUE)
popplaceb <- sf::st_buffer(popplacep, dist = units::set_units(10, "km"))

```

:::

## TerraClimate

TerraClimate data are provided in yearly NetCDF files, each of which contains monthly layers. We will read the data with the `terra` package and preprocess the data to extract the annual mean and sum of the bands by types of columns. 

```{r TerraClimate-read}
# wbd
ext_mainland <- c(xmin = -126, xmax = -64, ymin = 22, ymax = 51)
ext_mainland <- terra::ext(ext_mainland)

path_tc <- file.path("input", "terraClimate")
path_tc_files <- list.files(
  path = path_tc, pattern = "*.nc$",
  full.names = TRUE
)
path_tc_files
```

Fourteen variables are available in TerraClimate data. The table below is from [the TerraClimate website](http://www.climatologylab.org/terraclimate-variables.html).

| Variable | Description | Units |
|:--------:|:------------|:------|
| aet      | Actual Evapotranspiration, monthly total | mm |
| def      | Climate Water Deficit, monthly total | mm |
| PDSI     | Palmer Drought Severity Index, at end of month | unitless |
| pet      | Potential evapotranspiration, monthly total | mm |
| ppt      | Precipitation, monthly total | mm |
| q        | Runoff, monthly total | mm |
| soil     | Soil Moisture, total column - at end of month | mm |
| srad     | Downward surface shortwave radiation | W/m<sup>2</sup> |
| swe      | Snow water equivalent - at end of month | mm |
| tmax     | Max Temperature, average for month | C |
| tmin     | Min Temperature, average for month | C |
| vap      | Vapor pressure, average for month | kPa |
| vpd      | Vapor Pressure Deficit, average for month | kpa |
| ws       | Wind speed, average for month | m/s |

The variables can be categorized into two types: those that will be summed and those that will be averaged.

- Sum: `aet`, `def`, `pet`, `ppt`, `q`, `soil`, and `swe`.
- Mean: `PDSI`, `srad`, `tmax`, `tmin`, `vap`, `vpd`, and `ws`.

Following that rationale, we will preprocess the data by summing the first seven layers and averaging the rest of the layers. The code blocks below follow "split-apply-combine" strategy. Note that `terra::tapp` aggregates layers by its attributes such as time or custom indices. 

```{r TerraClimate-preprocess}
# some bands should be summed
bandnames <- c(
  "aet", "def", "PDSI", "pet", "ppt", "q", "soil", "srad",
  "swe", "tmax", "tmin", "vap", "vpd", "ws"
)
bandnames_sorted <- sort(bandnames)

# single nc file, yearly aggregation by fun value
# band for summation
bandnames_sum <- c("aet", "def", "pet", "ppt", "q", "soil", "swe")

# band for averaging
bandnames_avg <- c("PDSI", "srad", "tmax", "tmin", "vap", "vpd", "ws")

# mean: temporally marginal pixel mean (i.e., monthly -> yearly)
# sum: temporally marginal pixel sum (i.e., monthly -> yearly)
# Preprocessed data are stored in
tictoc::tic("sum: 7 layers")
netcdf_read_sum <-
  split(bandnames_sum, bandnames_sum) |>
  lapply(function(x) {
    grep(paste0("(", x, ")"), path_tc_files, value = TRUE)
  }) |>
  lapply(function(x) {
    terra::tapp(terra::rast(x, win = ext_mainland, snap = "out"), index = "years", fun = "sum")
  })
netcdf_read_sum <- Reduce(c, netcdf_read_sum)
tictoc::toc()
# 175.091 sec elapsed
# tapp: 185.288 sec
# tapp: 11.8G peak
names(netcdf_read_sum) <- rep(bandnames_sum, each = 23)
netcdf_read_sum

tictoc::tic("mean: 7 layers")
netcdf_read_mean <-
  split(bandnames_avg, bandnames_avg) |>
  lapply(function(x) {
    grep(paste0("(", x, ")"), path_tc_files, value = TRUE)
  }) |>
  lapply(function(x) {
    terra::tapp(terra::rast(x, win = ext_mainland, snap = "out"), index = "years", fun = "mean")
  }) |>
  Reduce(f = c, x = _)
tictoc::toc()
# 178.46 sec elapsed
names(netcdf_read_mean) <-
  sprintf("%s_%d", rep(bandnames_avg, each = 23), rep(2000:2022, 7))
netcdf_read_mean

```


We have 14 data elements for 23 years with 12 months each. Below demonstrates the summary of the data layers that were averaged. To leverage multiple cores in your machine, run `future::availableCores()` to check the number of cores and set the number of workers in `future::plan` accordingly. There are typically two logical cores in each physical core in modern central processing units. The number of workers should be set to up to the number of physical cores in the machine for optimal performance.

```{r multiraster}
doFuture::registerDoFuture()
future::plan(future::multicore, workers = 4L)

tic()
multi <-
grep(paste0("(", paste(bandnames_avg, collapse = "|"), ")"), path_tc_files, value = TRUE) %>%
chopin::par_multirasters(
  filenames = .,
  fun_dist = chopin::extract_at_buffer,
  combine = dplyr::bind_cols,
  points = popplacep2,
  surf = rast(), # ignored
  id = "GEOID",
  func = "mean",
  radius = 1e4
)
toc()
tic()
doFuture::registerDoFuture()
future::plan(future::multicore, workers = 8L)
grid_init <-
  chopin::par_make_gridset(
    popplacep2,
    mode = "grid",
    padding = 1e4,
    nx = 4L,
    ny = 2L
  )
multi_grid <-
  chopin::par_grid(
    grids = grid_init,
    fun_dist = chopin::extract_at_buffer,
    combine = dplyr::bind_rows,
    points = popplacep2,
    surf = netcdf_read_mean, # ignored
    id = "GEOID",
    func = "mean",
    radius = 1e4
  )
toc()

tic()
grid_init <-
  chopin::par_make_gridset(
    popplacep2,
    mode = "grid",
    padding = 1e4,
    nx = 2L,
    ny = 2L
  )
multi_gridf <-
  par_grid_furrr(
    grids = grid_init,
    fun_dist = chopin::extract_at_buffer,
    combine = dplyr::bind_rows,
    points = popplacep2,
    surf = netcdf_read_mean, # ignored
    id = "GEOID",
    func = "mean",
    radius = 1e4
  )
toc()


# single
tic()
single <-
exactextractr::exact_extract(
  netcdf_read_mean,
  sf::st_as_sf(popplaceb),
  fun = "mean",
  stack_apply = TRUE,
  force_df = TRUE,
  append_cols = c("GEOID")
)
toc()

multi %>%
  select(GEOID, contains("vpd")) %>%
  filter(!is.na(mean.vpd_1)) %>%
  arrange(GEOID) %>%
  .[1:5, -1] %>%
  rowSums(.)

single %>%
  arrange(GEOID) %>%
  select(GEOID, contains("vpd")) %>%
  .[1:5,]


```

> \[!CAUTION\] RStudio users will not be able to use `future::multicore` due to the limitation of the RStudio IDE. Use `future::multisession` instead.



> \[!NOTE\] This is a note.

> \[!TIP\] This is a tip. (Supported since 14 Nov 2023)

> \[!IMPORTANT\] Crutial information comes here.

> \[!CAUTION\] Negative potential consequences of an action. (Supported since 14 Nov 2023)

> \[!WARNING\] Critical content comes here.

::: {.panel-tabset}
## Download and preprocess

```{r}
#| label: populated places download

# populated places
# state <- tigris::states(year = 2022)
# statemain <-
#   state[!state$STUSPS %in% c("AK", "HI", "PR", "VI", "GU", "MP", "AS"), ]
# target_states <- statemain$GEOID

# popplace <-
#   lapply(target_states, function(x) tigris::places(x, year = 2022)) %>%
#   do.call(rbind, .)
# saveRDS(popplace, "./input/populated_place_2022.rds", compress = "xz")

```

## Read data 
```{r PRISM}
bils <- list.files("input", "bil$", recursive = TRUE, full.names = TRUE)
bilssds <- terra::rast(bils[-13])
popplace2 <- sf::st_transform(popplace, crs = terra::crs(bilssds))
popplaceb2 <- sf::st_transform(popplaceb, crs = terra::crs(bilssds))


```

:::

```{r}
system.time(
  exsingle <-
    exactextractr::exact_extract(
      bilssds,
      popplaceb2,
      fun = "mean",
      stack_apply = TRUE,
      force_df = TRUE,
      append_cols = "GEOID",
      max_cells_in_memory = 2.14e9
    )
)
# 2.14e9
#    user  system elapsed 
#  19.874   1.928  30.800 
# 1e8
#    user  system elapsed 
#  21.262   1.610  44.005 

exsinglep <-
  exactextractr::exact_extract(
    bilssds,
    popplace2,
    fun = "mean",
    stack_apply = TRUE,
    force_df = TRUE,
    append_cols = "GEOID"
  )


system.time(
  exgrid <-
    chopin::par_make_gridset(
      popplacep,
      mode = "grid",
      padding = 1e4,
      nx = 6L,
      ny = 3L
    )
)

exgrid <-
  chopin::par_make_gridset(
    popplacep2,
    mode = "grid",
    padding = 1e4,
    nx = 6L,
    ny = 3L
  )

doFuture::registerDoFuture()
future::plan(future::multicore, workers = 18L)
options(future.globals = TRUE)
system.time(
  exmulti <-
    chopin::par_grid(
      exgrid,
      fun_dist = chopin::extract_at_buffer,
      points = popplacep2,
      surf = bilssds,
      radius = units::set_units(1e4, "meter"),
      id = "GEOID",
      func = "mean"
    )
)
#    user  system elapsed
#  40.162  13.828  10.621

# no globals
options(future.globals = FALSE)
doFuture::registerDoFuture()
future::plan(future::multicore, workers = 18L)
system.time(
  exmulti <-
    chopin::par_grid(
      exgrid,
      fun_dist = chopin::extract_at_buffer,
      points = popplacep2,
      surf = bilssds,
      radius = units::set_units(1e4, "meter"),
      id = "GEOID",
      func = "mean"
    )
)
#    user  system elapsed 
#  39.737  14.437  11.368 

```


## Larger buffer sizes

```{r}
# make buffers
popplaceb5 <- sf::st_buffer(popplacep, dist = units::set_units(50, "km")) %>%
  sf::st_transform(terra::crs(bilssds))

system.time(
  exsingle5 <-
    exactextractr::exact_extract(
      bilssds,
      popplaceb5,
      fun = "mean",
      stack_apply = TRUE,
      force_df = TRUE,
      append_cols = "GEOID",
      max_cells_in_memory = 2.14e9
    )
)

system.time(
  exgrid5k <-
    chopin::par_make_gridset(
      popplacep,
      mode = "grid",
      padding = 5e4,
      nx = 6L,
      ny = 3L
    )
)

exgrid5k <-
  chopin::par_make_gridset(
    popplacep2,
    mode = "grid",
    padding = 5e4,
    nx = 6L,
    ny = 3L
  )


options(future.globals = FALSE)
doFuture::registerDoFuture()
future::plan(future::multisession, workers = 18L)
system.time(
  exmulti5k <-
    chopin::par_grid(
      exgrid5k,
      fun_dist = chopin::extract_at_buffer,
      points = popplacep,
      surf = bils[-13],
      radius = units::set_units(5e4, "meter"),
      id = "GEOID",
      func = "mean"
    )
)
```

```{r}
## generate 1km grid points in the southeastern US States
stt <- tigris::states(year = 2020)
targ_states <- c("NC", "SC", "GA", "FL", "AL", "MS", "TN", "LA", "AR")
stt_targ <- stt[stt$STUSPS %in% targ_states, ]
plot(stt_targ$geometry)
st_bbox(stt_targ)
stt_t <- sf::st_transform(stt_targ, "EPSG:5070")
stt_g <- sf::st_sample(stt_t, type = "regular", 1204934)
stt_g <- sf::st_as_sf(stt_g)
stt_g$pid <- seq(1, length(stt_g))
```

```{r}
stt_gb <- sf::st_buffer(stt_g, units::set_units(5, "km"))

tic()
single_2m <-
exactextractr::exact_extract(
  netcdf_read_sum,
  stt_gb,
  fun = "mean",
  stack_apply = TRUE,
  force_df = TRUE,
  max_cells_in_memory = 2.14e9
)
toc()

stt_gbg <-
  chopin::par_make_gridset(
    stt_g,
    mode = "grid",
    padding = 5e3,
    nx = 5L,
    ny = 5L
  )


doFuture::registerDoFuture()
future::plan(future::multicore, workers = 21L)
system.time(
  stt5k <-
    chopin::par_grid(
      stt_gbg,
      fun_dist = chopin::extract_at_poly,
      poly = stt_gb,
      surf = netcdf_read_sum,
      id = "pid",
      func = "mean"
    )
)

doFuture::registerDoFuture()
future::plan(future::multisession, workers = 21L)
system.time(
  stt5ks <-
    chopin::par_grid(
      stt_gbg,
      fun_dist = chopin::extract_at_poly,
      poly = stt_gb,
      surf = st_as_stars(netcdf_read_mean),
      id = "pid",
      func = "mean"
    )
)
```

### Addendum 1: even finely resolved dataset

Even with a finely resolved dataset, the extraction process can be expedited with the `chopin` package. We demonstrate the extraction process with the CropScape dataset which has a resolution of 30 meters. In this case, we use `frac` option of `exact_extract` which tabulates the fraction of the area of the cell category that is covered by the polygon.



```{r}
tic()
bg <- sf::st_read("~/Blockgroups_2020.gpkg")
toc()

stt <- tigris::states(year = 2020)

## extract prism at bg
system.time(
  exsingle <-
    exactextractr::exact_extract(
      bilssds,
      bgsf %>% dplyr::filter(!STATEFP %in% c("02", "15", "72", "66", "78", "60", "69")),
      fun = "mean",
      stack_apply = TRUE,
      force_df = TRUE,
      append_cols = "GEOID",
      max_cells_in_memory = 2.14e9
    )
)
#    user  system elapsed 
# 116.200   2.951 131.969

## extract prism by par_hierarchy
tic()
bgsf <- sf::st_read("~/Blockgroups_2020.gpkg")
toc()

nmain <- c("AS", "AK", "HI", "PR", "VI", "GU", "MP")
stt_nmain <- stt[!stt$STUSPS %in% nmain, ]


options(future.globals = FALSE, future.globals.maxSize = +Inf)
doFuture::registerDoFuture()
# cl <- mirai::make_cluster(8)

# future::plan("multicore", workers = 6L)
future::plan(multicore, workers = 20L)
system.time(
  exhierarchy <-
    chopin::par_hierarchy(
      bgsf,
      split_level = "STATEFP",
      fun_dist = chopin::extract_at_poly,
      polys = bgsf,
      surf = bils,
      id = "GEOID"
    )
)

#  user  system elapsed 
# 7.255   2.367 205.995
# multicore (reflow)
  #  user  system elapsed 
  # 1.860   0.408 160.795 
# layer: sequential, multicore
#    user  system elapsed 
# 152.557   3.123 157.137 
```




### Addendum 2: which is faster? Stacked vs file-based parallelization

For faster computation, there are two strategies. One is parallelization which is implemented in `chopin` for example. The other strategy is to stack rasters and extract values at once. Adjustment of arguments in `exactextractr::exact_extract` will benefit many people who are dealing with sizable data that are able to be stacked. We compare the two strategies in terms of computation time.

Before proceeding, users need to consider the hardware specification. For example, memory and storage to leverage the maximal performance of exact_extract. Specifically speaking, memory capacity is crucial to store the stacked rasters in memory rather than to read the proxy of rasters from the disk as implemented in `terra`. `max_cells_in_memory` is a key argument to control the memory usage. The maximum possible value for this argument is $2^{31} - 1 = 2,147,483,647$, roughly `2.147e9`, as applied in the example above. As memory bandwidth is much faster than disk I/O, the stacked rasters with high `max_cells_in_memory` applied will run faster than file-based parallelization or the extraction with lower value of `max_cells_in_memory`. The performance does not come with a cost. The memory-intensive setting is not suitable for the system with limited memory, for example, in consumer laptops with \~16 GB of RAM.

More tips to save time and memory:

- Always consider stacking rasters when you have a large number of separate raster files with the same resolution and extent. Reading multiple raster files with `terra::rast` will automatically stack them into a single `SpatRaster` object.

> \[!NOTE\] Stacking rasters may take large amount of memory. Users need to consider the memory capacity of the system before stacking rasters.


- Read vector data with `sf` package at first. It is a bit faster than `terra` and will save time for processing as `exactextractr` is designed to work with `sf` objects for vector inputs.
- If users want to use `exactextractr` for the raster-vector overlay, pre-cropping the raster data may not help saving time for processing. This is because `exactextractr` will read the raster data with the extent of the vector data *ad hoc*.
- If your analysis does not require the high precision of vector data, simplification of geometries (e.g., using `rmapshaper`) will result in considerable time savings.


### References

-   [Garnett, R. (2023). Geospatial distributed processing with furrr](https://posit.co/blog/geospatial-distributed-processing-with-furrr/)
-   [Dyba, K. (n.d.). Parallel raster processing in *stars*](https://kadyb.github.io/stars-parallel/Tutorial.html)



```{r}
library(future)
library(future.batchtools)

doFuture::registerDoFuture()

plan(
  list(
    tweak(
      batchtools_slurm,
      template = "./tools/slurm_test/template_slurm.tmpl",
      resources =
      list(
        memory = 4, # per cpu
        ncpus = 24,
        ntasks = 1,
        walltime = 600,
        email = "songi2@nih.gov",
        log.file =
        sprintf(
          "~/rtest/test_%s.log",
          strftime(Sys.time(), "%Y%m%d_%H%M%S")),
        error.file =
        sprintf(
          "~/rtest/test_%s.error",
          strftime(Sys.time(), "%Y%m%d_%H%M%S")),
        partition = ""
        )
      )
    ,
    multicore
  )
)


library(furrr)
xx %<-% furrr::future_map(
  .x = 1:10,
  .f = ~{
    Sys.sleep(1)
    .x
  },
  .options = furrr::furrr_options(packages = c("furrr"))
)
value(xx)


```